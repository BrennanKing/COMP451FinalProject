{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":4.669361,"end_time":"2024-10-10T13:05:46.686069","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-10T13:05:42.016708","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Here we go, time to get cracking with an aggregate of models.","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\n\nimport pandas as pd\nimport polars as pl\nimport pandas as pd\nimport numpy as np\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nimport random\n\nimport kaggle_evaluation.jane_street_inference_server","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-12-12T19:23:02.385258Z","iopub.execute_input":"2024-12-12T19:23:02.385702Z","iopub.status.idle":"2024-12-12T19:23:08.748558Z","shell.execute_reply.started":"2024-12-12T19:23:02.385667Z","shell.execute_reply":"2024-12-12T19:23:08.747792Z"},"papermill":{"duration":1.223703,"end_time":"2024-10-10T13:05:45.825911","exception":false,"start_time":"2024-10-10T13:05:44.602208","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setting a random seed so that we can compare how\n# changes in the code are changing our score.\n# We don't want randomness to be the reason for score changes.\ndef seed_randomness(seed):\n    # We have to set it for both python and numpy\n    random.seed(seed)\n    np.random.seed(seed)\n\nseed_randomness(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T19:23:08.750313Z","iopub.execute_input":"2024-12-12T19:23:08.751287Z","iopub.status.idle":"2024-12-12T19:23:08.756013Z","shell.execute_reply.started":"2024-12-12T19:23:08.751232Z","shell.execute_reply":"2024-12-12T19:23:08.755124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**LOADING DATA**\n\nWhen training the model, we kept running into issues with the notebook trying to allocate more memory than was available. After some looking around online, we found a function that reduces the memory usage of a pandas dataframe by going through each of its columns and opting for smaller data types wherever possible. The code was found from: https://www.machinelearningplus.com/data-manipulation/how-to-reduce-the-memory-size-of-pandas-data-frame/","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':  # for integers\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:  # for floats.\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n    print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T19:23:08.757078Z","iopub.execute_input":"2024-12-12T19:23:08.757338Z","iopub.status.idle":"2024-12-12T19:23:08.770465Z","shell.execute_reply.started":"2024-12-12T19:23:08.757312Z","shell.execute_reply":"2024-12-12T19:23:08.769579Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = []\nMAX_TIME_ID = 967\n# Due to time and memory constraints, we can't look at all of the training data\n# Look at the end of the data set because it shows the responder trends more clearly\n# (it also has fewer missing values)\nfor i in range(6, 10):\n    train=pl.read_parquet(f\"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id={i}/part-0.parquet\")\n    train=train.to_pandas()\n    # Feauture engineering: create new features based on time_id\n    # (shows how far through the day we are)\n    # sin and cos are cyclic so they wrap around to make the end of one day the same and the start of the next\n    train['sin_time'] = np.sin(2*np.pi*train['time_id']/MAX_TIME_ID)\n    train['cos_time'] = np.cos(2*np.pi*train['time_id']/MAX_TIME_ID)\n    train['two_sin_time'] = np.sin(8*np.pi*train['time_id']/MAX_TIME_ID)\n    train['two_cos_time'] = np.cos(8*np.pi*train['time_id']/MAX_TIME_ID)\n    #train['four_sin_time']=np.sin(8*np.pi*train['time_id']/MAX_TIME_ID)\n    #train['four_cos_time']=np.cos(8*np.pi*train['time_id']/MAX_TIME_ID)\n    \n    # Reduce memory usage\n    train = reduce_mem_usage(df=train)\n    # Storing data in the list\n    data.append(train)\n\n# Concatenate all of the pandas objects into one\ntrain = pd.concat(data)\n\n# Remove the stuff that we don't need anymore\ndel data\ngc.collect\n\n# Here is the ordering for the features that we want to use for training\ntraining_features = ['symbol_id','sin_time','cos_time','two_sin_time','two_cos_time'] + [f'feature_0{i}' if i<10 else f'feature_{i}' for i in range(79)]\ntrain = train[['responder_6'] + training_features]\n\n# Let's see what the start of the training data is looking like\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T19:23:08.773235Z","iopub.execute_input":"2024-12-12T19:23:08.773712Z","iopub.status.idle":"2024-12-12T19:24:15.183500Z","shell.execute_reply.started":"2024-12-12T19:23:08.773682Z","shell.execute_reply":"2024-12-12T19:24:15.182499Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**TRAINING THE MODEL**\n\nWe will be using several open source libraries that can each train up specific machine learning models. The libraries that we have chosen are xgboost, lightgbm, and catboost.\n\nHere are the parameters for each of them that we have landed on through reading the library documentation and doing iterative hyperparameter tuning:","metadata":{}},{"cell_type":"code","source":"lgb_params={'boosting_type': 'gbdt','metric':'rmse','random_state':2025,'max_depth':10,'learning_rate':0.1,'n_estimators':120,'colsample_bytree':0.6,'colsample_bynode':0.6,'reg_alpha': 0.2,'reg_lambda':5,'extra_trees':True,'num_leaves':64,'max_bin':255,'device':'gpu','gpu_use_dp':True,}\ncat_params={'task_type':'GPU','random_state':2025,'eval_metric':'RMSE','bagging_temperature':0.50,'iterations':200,'learning_rate':0.1,'max_depth':12,'l2_leaf_reg':1.25,'min_data_in_leaf':24,'random_strength':0.25,}\nxgb_params={'random_state': 2025, 'n_estimators': 125,'learning_rate':0.1,'max_depth':10,'reg_alpha':0.08,'reg_lambda':0.8,'subsample':0.95,'colsample_bytree':0.6,'min_child_weight':3,'tree_method':'hist','device':'cuda',}\n\nlgb=LGBMRegressor(**lgb_params)\nlgb.fit(train[training_features].values,train['responder_6'].values)\n\ncat=CatBoostRegressor(**cat_params)\ncat.fit(train[training_features].values,train['responder_6'].values)\n\nxgb=XGBRegressor(**xgb_params)\nxgb.fit(train[training_features].values,train['responder_6'].values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T19:24:15.184841Z","iopub.execute_input":"2024-12-12T19:24:15.185633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The evaluation API requires that you set up a server which will respond to inference requests. We have already defined the server; you just need write the predict function. When we evaluate your submission on the hidden test set the client defined in `jane_street_gateway` will run in a different container with direct access to the hidden test set and hand off the data timestep by timestep.\n\n\n\nYour code will always have access to the published copies of the files.","metadata":{"papermill":{"duration":0.002051,"end_time":"2024-10-10T13:05:45.83073","exception":false,"start_time":"2024-10-10T13:05:45.828679","status":"completed"},"tags":[]}},{"cell_type":"code","source":"lags_ : pl.DataFrame | None = None\n\n\n# Replace this function with your inference code.\n# You can return either a Pandas or Polars dataframe, though Polars is recommended.\n# Each batch of predictions (except the very first) must be returned within 1 minute of the batch features being provided.\ndef predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n    \"\"\"Make a prediction.\"\"\"\n\n    global lgb, xgb, cat\n    # All the responders from the previous day are passed in at time_id == 0. We save them in a global variable for access at every time_id.\n    # Use them as extra features, if you like.\n    global lags_\n    if lags is not None:\n        lags_ = lags\n\n    predictions = test.select(\n        'row_id',\n        pl.lit(0.0).alias('responder_6'),\n    )\n    test=test.to_pandas()\n    test['sin_time']=np.sin(2*np.pi*test['time_id']/MAX_TIME_ID)\n    test['cos_time']=np.cos(2*np.pi*test['time_id']/MAX_TIME_ID)\n    test['two_sin_time']=np.sin(4*np.pi*test['time_id']/MAX_TIME_ID)\n    test['two_cos_time']=np.cos(4*np.pi*test['time_id']/MAX_TIME_ID)\n    #test['four_sin_time']=np.sin(8*np.pi*test['time_id']/MAX_TIME_ID)\n    #test['four_cos_time']=np.cos(8*np.pi*test['time_id']/MAX_TIME_ID)\n    test=test.fillna(-1)\n    test=test[training_features]\n    eps=1e-10\n    test_preds=0.55*lgb.predict(test)+0.2*cat.predict(test)+0.25*xgb.predict(test)\n    test_preds=np.clip(test_preds,-5+eps,5-eps)\n    predictions = predictions.with_columns(pl.Series('responder_6', test_preds.ravel()))\n    return predictions","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.015917,"end_time":"2024-10-10T13:05:45.848958","exception":false,"start_time":"2024-10-10T13:05:45.833041","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"When your notebook is run on the hidden test set, inference_server.serve must be called within 15 minutes of the notebook starting or the gateway will throw an error. If you need more than 15 minutes to load your model you can do so during the very first `predict` call, which does not have the usual 1 minute response deadline.","metadata":{"papermill":{"duration":0.00196,"end_time":"2024-10-10T13:05:45.853279","exception":false,"start_time":"2024-10-10T13:05:45.851319","status":"completed"},"tags":[]}},{"cell_type":"code","source":"inference_server = kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n        )\n    )","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.308219,"end_time":"2024-10-10T13:05:46.163573","exception":false,"start_time":"2024-10-10T13:05:45.855354","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}